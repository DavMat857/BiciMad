{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La empresa gestora de BiciMad se encuentra ante un problema que busca resolver a través de un estudio centrado en las edades y el tiempo promedio de viaje de sus usuarios. El propósito de este estudio es doble: por un lado, utilizar estos datos como criterios para su expansión a nuevas zonas y, por otro lado, establecer una promoción especial que ofrece un descuento a los usuarios.\n",
    "\n",
    "Para activar esta promoción, se ha decidido establecer un requisito específico: los usuarios deberán alcanzar un tiempo medio de uso determinado. De esta manera, se busca no solo incentivar un mayor uso de BiciMad, sino también promover el disfrute de los servicios que la empresa ofrece.\n",
    "\n",
    "Este análisis de las edades y los tiempos promedio de viaje proporcionará a BiciMad información valiosa para la toma de decisiones estratégicas. Con estos datos, la empresa podrá identificar áreas con una demanda más significativa, optimizar su expansión y, en última instancia, brindar un servicio más completo y adaptado a las necesidades de sus usuarios.\n",
    "\n",
    "#### Para ello usaremos el siguiente script\n",
    "\n",
    "`conteo_rango_edades.py`: lee una serie de base de datos en las que se encuentre información relacionada con ageRange, y se realiza un diagrama de los sectores, además calcula el tiempo medio de viaje.\n",
    "\n",
    " __observación__: para este análisis se han realizados dos caso:\n",
    " \n",
    " <ol>\n",
    " <li> Sin filtrado de datos: tomando directamente los datos </li>\n",
    " <li> Filtrando datos que consideramos poco importantes: una limpieza de datos, que se resume en eliminar los usuarios con edad no registrado </li>\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python3: can't open file 'd:\\\\Documentos\\\\BiciMad-1\\\\conteo_rango_edades.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "python3 conteo_rango_edades.py \"DatosDeUso_12_2020.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a abrir dos imágenes una con un filtrado de datos y otro sin él"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'comparacion_rango_edades.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpimg\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# Cargar las imágenes\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m imagen1 \u001b[39m=\u001b[39m mpimg\u001b[39m.\u001b[39;49mimread(\u001b[39m'\u001b[39;49m\u001b[39mcomparacion_rango_edades.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m imagen2 \u001b[39m=\u001b[39m mpimg\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mcomparacion_rango_edades_filtrado.png\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Crear una figura y agregar subplots\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\matplotlib\\image.py:1563\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parse\u001b[39m.\u001b[39murlparse(fname)\u001b[39m.\u001b[39mscheme) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1557\u001b[0m     \u001b[39m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1561\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m         )\n\u001b[1;32m-> 1563\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[0;32m   1564\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   1565\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m             pil_to_array(image))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\PIL\\ImageFile.py:105\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[1;34m(self, fp, filename)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodermaxblock \u001b[39m=\u001b[39m MAXBLOCK\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m is_path(fp):\n\u001b[0;32m    104\u001b[0m     \u001b[39m# filename\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(fp, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fp\n\u001b[0;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'comparacion_rango_edades.png'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Cargar las imágenes\n",
    "imagen1 = mpimg.imread('comparacion_rango_edades.png')\n",
    "imagen2 = mpimg.imread('comparacion_rango_edades_filtrado.png')\n",
    "\n",
    "# Crear una figura y agregar subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Mostrar las imágenes en los subplots\n",
    "axes[0].imshow(imagen1)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Sin filtrar datos')\n",
    "\n",
    "axes[1].imshow(imagen2)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Filtrando datos')\n",
    "\n",
    "# Ajustar la distribución de los subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar la figura con las imágenes\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusiones:\n",
    "\n",
    "* La mayoría de los usuarios no tienen registrado la edad. Por lo tanto, se sugiere considerar la posibilidad de realizar una promoción o incentivo para obtener más datos sobre los usuarios y poder personalizar las ofertas y servicios.\n",
    "\n",
    "* Observando la imagen proporcionada en la celda anterior, se puede notar que la mayoría de los usuarios registrados se encuentran en los grupos 4 y 5, correspondientes a edades entre 27 y 65 años. Esto indica que BiciMad tiene una buena aceptación en este rango de edad y puede enfocar sus estrategias de marketing y expansión en esta audiencia.\n",
    "\n",
    "* Los grupos 1 y 2, correspondientes a usuarios menores de edad, muestran una baja representación en los registros. Para captar y fidelizar a este grupo de usuarios jóvenes, se sugiere considerar la implementación de viajes gratuitos o bonos especiales que les brinden beneficios exclusivos. Esto ayudaría a generar interés y crear una base de usuarios sólida desde temprana edad."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En relación al tiempo medio de viaje:\n",
    "\n",
    "Tenemos el dato de que el tiempo medio de viaje es 1056.55 segundos, que son 17.61 minutos.\n",
    "\n",
    "Teniendo en cuenta esta información, la empresa ha decidido poner en marcha una estrategia de descuento como incentivo para sus usuarios. La idea es utilizar el tiempo medio de viaje como criterio para determinar qué usuarios podrán acceder a este descuento especial. Aquellos usuarios que cumplan con el tiempo medio establecido serán beneficiados con una reducción en el costo de sus viajes.\n",
    "\n",
    "Esta estrategia de descuento basada en el tiempo medio de viaje tiene múltiples ventajas para la empresa. En primer lugar, fomenta un mayor uso de BiciMad al incentivar a los usuarios a realizar viajes más prolongados. Esto a su vez contribuye a un mayor tiempo de uso de las bicicletas y, potencialmente, a un aumento en los ingresos generados por el servicio.\n",
    "\n",
    "Además, esta estrategia permite recopilar información valiosa sobre los hábitos de uso de los usuarios. Al monitorear el tiempo medio de viaje y analizar los datos resultantes, la empresa puede obtener insights sobre las preferencias de los usuarios, los patrones de movilidad y las áreas de mayor interés. Esta información puede ser utilizada para mejorar la planificación de rutas, optimizar la distribución de estaciones y enfocar los esfuerzos de expansión en las áreas de mayor demanda."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los problemas que enfrenta la empresa gestora de BiciMad es la falta de visibilidad y comprensión sobre las estaciones más concurridas y los trayectos más populares a lo largo del día. Sin esta información, resulta difícil tomar decisiones estratégicas para mejorar la oferta de servicios y optimizar la ubicación de las estaciones en función de la demanda real. Para ello usaremos el script `mapa_dia.py`.\n",
    "\n",
    "Primero, el programa guarda las posiciones ([longitude,latitude]) y los identificadores de las estaciones activas para el día específico que se ha introducido como valor de entrada, y hace lo propio con las variables idplug_station (estación de enganche) e idunplug_station (estación de desenganche) del fichero que guarda los movimientos de los usuarios conectados a la red de Bicimad.\n",
    "\n",
    "Con esta información se evalúa cuáles han sido las estaciones más concurridas a lo largo del día introducido, y utilizando la librería de visualización geoespacial folium presentamos esta información en un mapa de forma que se pueda acceder a la información de enganches y desenganches de todas las estaciones, así como proporcionar una representación que (mediante una escala de color) permita conocer cuáles son las estaciones de mayor interés. Es importante remarcar que el formato de los archivos JSON debe ser el que siguen los archivos del año 2020 (véanse DatosDeUso_12_2020.json y SituacionesEstaciones12_2020.json como posibles referencias) de libre acceso en la página web de la EMT de Madrid https://opendata.emtmadrid.es/Datos-estaticos/Datos-generales.\n",
    "\n",
    "Además, se han delineado los trayectos con más apariciones en el conjunto de datos de uso de los cuáles se ha seleccionado top = 500 para visualizar las 500 rutas más concurridas a lo large de ese día, junto con la información de las estaciones explicada en el párrafo anterior. Si se desea, este valor puede cambiarse dentro del código para reducir o aumentar el número de rutas a considerar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/d:/Documentos/BiciMad-1/DatosDeuso12_2020.json\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Input path does not exist: file:/d:/Documentos/BiciMad-1/DatosDeuso12_2020.json\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m conf \u001b[39m=\u001b[39m SparkConf()\u001b[39m.\u001b[39msetAppName(\u001b[39m\"\u001b[39m\u001b[39mmapaBicimad\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m sc \u001b[39m=\u001b[39m SparkContext(conf\u001b[39m=\u001b[39mconf) \n\u001b[1;32m----> 7\u001b[0m main(sc)\n",
      "File \u001b[1;32md:\\Documentos\\BiciMad-1\\scripts\\mapa_dia.py:67\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(sc, usage_file, stations_file, day, outfile, top)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m# Extraemos la información de los enganches y desenganches\u001b[39;00m\n\u001b[0;32m     65\u001b[0m result \u001b[39m=\u001b[39m filtered_usage_rdd\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: {\u001b[39m'\u001b[39m\u001b[39midplug_station\u001b[39m\u001b[39m'\u001b[39m: json\u001b[39m.\u001b[39mloads(x)[\u001b[39m'\u001b[39m\u001b[39midplug_station\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     66\u001b[0m                                         \u001b[39m'\u001b[39m\u001b[39midunplug_station\u001b[39m\u001b[39m'\u001b[39m: json\u001b[39m.\u001b[39mloads(x)[\u001b[39m'\u001b[39m\u001b[39midunplug_station\u001b[39m\u001b[39m'\u001b[39m]})\n\u001b[1;32m---> 67\u001b[0m result1 \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39;49mcollect()\n\u001b[0;32m     68\u001b[0m trips \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlist\u001b[39m(x\u001b[39m.\u001b[39mvalues()))\u001b[39m.\u001b[39mcollect()\n\u001b[0;32m     69\u001b[0m trips_count \u001b[39m=\u001b[39m count_occurrences(trips)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mctx\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[0;32m   1815\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/d:/Documentos/BiciMad-1/DatosDeuso12_2020.json\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.IOException: Input path does not exist: file:/d:/Documentos/BiciMad-1/DatosDeuso12_2020.json\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r\n\t... 34 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from scripts.mapa_dia import main\n",
    "\n",
    "conf = SparkConf().setAppName(\"mapaBicimad\")\n",
    "sc = SparkContext(conf=conf) \n",
    "main(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizamos el mapa creado\n",
    "import webbrowser\n",
    "\n",
    "file_path = \"mapa_madrid.html\"\n",
    "webbrowser.open(file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Solución**:\n",
    "\n",
    "Para abordar este problema, se ha desarrollado un programa que utiliza datos de posicionamiento y movimientos de los usuarios para identificar las estaciones más concurridas y visualizar esta información en un mapa interactivo. La solución se basa en los siguientes pasos:\n",
    "\n",
    "Recopilación de datos: El programa recopila información sobre las posiciones (longitude, latitude) y los identificadores de las estaciones activas para el día específico proporcionado como entrada. Además, se obtienen los datos de las estaciones de enganche y desenganche de los usuarios conectados a la red de BiciMad.\n",
    "\n",
    " Evaluación de las estaciones más concurridas: Utilizando los datos recopilados, se analiza cuáles han sido las estaciones más concurridas a lo largo del día. Esto se realiza teniendo en cuenta tanto las estaciones de enganche como las de desenganche de los usuarios. Esta evaluación proporciona una visión clara de las áreas de mayor interés y demanda en la red de BiciMad.\n",
    "\n",
    "Visualización en un mapa interactivo: Se utiliza la librería de visualización geoespacial folium para presentar la información recopilada en un mapa interactivo. El mapa muestra todas las estaciones y permite acceder a la información de enganches y desenganches de cada una. Además, se utiliza una escala de color para resaltar las estaciones de mayor interés, lo que facilita la identificación visual de las áreas más concurridas.\n",
    "\n",
    "Delineación de trayectos populares: Se seleccionan los trayectos más frecuentes en el conjunto de datos de uso. El programa permite ajustar el parámetro \"top\" para determinar el número de rutas a considerar. Por defecto, se visualizan las 500 rutas más concurridas del día. Esta información complementa la visualización de las estaciones, brindando una comprensión más completa de los patrones de movimiento de los usuarios.\n",
    "\n",
    "Esta solución permite a la empresa gestora de BiciMad obtener una visión clara de las estaciones más concurridas y los trayectos populares, lo que facilita la toma de decisiones estratégicas. Con esta información, la empresa puede optimizar la ubicación de las estaciones, ajustar la oferta de servicios en áreas de mayor demanda y mejorar la experiencia global de los usuarios. Además, la visualización en un mapa interactivo brinda una forma intuitiva de acceder y comprender la información, facilitando la identificación de patrones y tendencias importantes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texto rutas lentas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from scripts.rutas_lentas import obtener_velocidades\n",
    "\n",
    "# Creamos la sesión de Spark\n",
    "spark_session = SparkSession.builder.appName(\"RutasLentas\").config(\"spark.executor.memory\", \"8g\").getOrCreate()\n",
    "spark_session.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Elegimos el mes que vamos a analizar\n",
    "ruta_movements = r\"datos\\movements\\202012_movements.json\"\n",
    "ruta_stations = r\"datos\\stations\\202012_stations.json\"\n",
    "\n",
    "velocidades = obtener_velocidades(ruta_movements, ruta_stations, spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o163.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 6.0 failed 1 times, most recent failure: Lost task 6.0 in stage 6.0 (TID 55) (DESKTOP-VSFN77U executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:433)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:812)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1120)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implFlush(BufferedOutputStream.java:252)\r\n\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:240)\r\n\tat java.base/java.io.DataOutputStream.flush(DataOutputStream.java:129)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:433)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:812)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1120)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implFlush(BufferedOutputStream.java:252)\r\n\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:240)\r\n\tat java.base/java.io.DataOutputStream.flush(DataOutputStream.java:129)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m velocidades\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o163.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 6.0 failed 1 times, most recent failure: Lost task 6.0 in stage 6.0 (TID 55) (DESKTOP-VSFN77U executor driver): java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:433)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:812)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1120)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implFlush(BufferedOutputStream.java:252)\r\n\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:240)\r\n\tat java.base/java.io.DataOutputStream.flush(DataOutputStream.java:129)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: java.net.SocketException: Connection reset by peer\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implWrite(NioSocketImpl.java:413)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.write(NioSocketImpl.java:433)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$2.write(NioSocketImpl.java:812)\r\n\tat java.base/java.net.Socket$SocketOutputStream.write(Socket.java:1120)\r\n\tat java.base/java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:125)\r\n\tat java.base/java.io.BufferedOutputStream.implFlush(BufferedOutputStream.java:252)\r\n\tat java.base/java.io.BufferedOutputStream.flush(BufferedOutputStream.java:240)\r\n\tat java.base/java.io.DataOutputStream.flush(DataOutputStream.java:129)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:443)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\r\n"
     ]
    }
   ],
   "source": [
    "velocidades.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "\n",
    "escribir conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1554ca09010399fa294997e9b93fe3492fe8acfeb68a288697aa104cf34db993"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
